# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_chinese, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_chinese,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(mlr3verse)
library(rpart.plot)
library(paradox)
library(future)
set.seed(2022)
learner = lrn("classif.naive_bayes",
eps        = to_tune(-1,1),
laplace  = to_tune(0, 5),
threshold  = to_tune(-1,1)
)
at = auto_tuner(
method = tnr("random_search"),
learner = learner,
resampling = rsmp("cv", folds = 3),
measure = msr("classif.acc"),
terminator = trm("evals", n_evals = 20)
)
task = task_danish
task = task_chinese
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
rr = resample(task, at, outer_resampling, store_models = TRUE)
task = task_chinese_0
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
extract_inner_tuning_results(rr)
#rr = resample(task, at, outer_resampling, store_models = TRUE)
#extract_inner_tuning_results(rr)
#rr$aggregate()
at$train(task_chinese)
#rr = resample(task, at, outer_resampling, store_models = TRUE)
#extract_inner_tuning_results(rr)
#rr$aggregate()
at$train(task_chinese_0)
library(mlr3verse)
library(rpart.plot)
library(paradox)
library(future)
set.seed(2022)
learner = lrn("classif.naive_bayes",
laplace  = to_tune(0, 5)
)
at = auto_tuner(
method = tnr("random_search"),
learner = learner,
resampling = rsmp("cv", folds = 3),
measure = msr("classif.acc"),
terminator = trm("evals", n_evals = 20)
)
task = task_chinese_0
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
extract_inner_tuning_results(rr)
rr$aggregate()
at$train(task_chinese_0)
saveRDS(at,"nb_chinese.rds")
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
loaded_tuned_model <- readRDS("nb_chinese.rds")
english <- english %>%
drop_na()
english %>%
anyNA()
english_results <- english%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = english))
# Print predictions
english_results %>%
slice_head(n = 5)
# Confusion matrix
#install.packages("yardstick")
library(yardstick)
english_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
english_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = english_results, truth = emotion, estimate = ...2) %>%
summary()
spanish <-read_csv("Spanish.csv")
spanish = subset(spanish,select = -c(nationality,ID))
spanish$emotion = as.factor(spanish$emotion)
loaded_tuned_model <- readRDS("nb_chinese.rds")
spanish <- spanish %>%
drop_na()
spanish %>%
anyNA()
spanish_results <- spanish%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = spanish))
# Print predictions
spanish_results %>%
slice_head(n = 5)
# Confusion matrix
spanish_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
spanish_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = spanish_results, truth = emotion, estimate = ...2) %>%
summary()
english <-read_csv("English.csv")
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
loaded_tuned_model <- readRDS("nb_chinese.rds")
english <- english %>%
drop_na()
english %>%
anyNA()
english_results <- english%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = english))
# Print predictions
english_results %>%
slice_head(n = 5)
# Confusion matrix
english_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
english_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = english_results, truth = emotion, estimate = ...2) %>%
summary()
danish <-read_csv("Danish.csv")
danish = subset(danish,select = -c(nationality,ID))
danish$emotion = as.factor(danish$emotion)
loaded_tuned_model <- readRDS("nb_chinese.rds")
danish <- danish %>%
drop_na()
danish %>%
anyNA()
danish_results <- danish%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = danish))
# Print predictions
danish_results %>%
slice_head(n = 5)
# Confusion matrix
danish_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
danish_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = danish_results, truth = emotion, estimate = ...2) %>%
summary()
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(312,kind = "L'Ecuyer-CMRG")
spanish <-read_csv("Spanish.csv",show_col_types = FALSE)
spanish = subset(spanish,select = -c(nationality,ID))
spanish$emotion = as.factor(spanish$emotion)
#install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
spanish_1 = spanish %>% drop_na()
set.seed(2022)
boruta_spanish_train <- Boruta(emotion ~ . , data= spanish_1, doTrace=2) # perform Boruta search
print(boruta_spanish_train)
boruta_spanish <- TentativeRoughFix(boruta_spanish_train)
print(boruta_spanish)
boruta_signif <- names(boruta_spanish_train$finalDecision[boruta_spanish_train$finalDecision %in% c("Confirmed", "Tentative")]) # collect Confirmed and Tentative variables
print(boruta_signif)
plot(boruta_spanish, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_spanish$ImpHistory),function(i)
boruta_spanish$ImpHistory[is.finite(boruta_spanish$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_spanish$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_spanish$ImpHistory), cex.axis = 0.7)
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
task_spanish_0 <- TaskClassif$new(id = "spanish", backend = spanish_1,
target = "emotion")
set.seed(2022)
rf_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
rf_classif$train(task_spanish_0)
rf_exp_spanish <- explain_mlr3(rf_classif,
data = spanish_1,
y = spanish_1$emotion,
label = "ranger explainer spanish", colorize = FALSE)
varimp_spanish <- model_parts(rf_exp_spanish,
type = "difference")
plot(varimp_spanish, show_boxplots = TRUE)
variable_importance = as.data.frame(varimp_spanish)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance$variable),11)
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob")
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_spanish, resampling = rdesc)
res_featureless$prediction()$confusion
print('Featureless');print(res_featureless$aggregate(mes))
# Penalized Generalized Linear Models
library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_spanish, resampling = rdesc)
res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
tree = lrn("classif.rpart")
tree = as_learner(imputer %>>% tree)
set.seed(2022)
res_tree = resample(learner = tree, task = task_spanish, resampling = rdesc)
res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
rf = lrn("classif.ranger",num.trees = 500)
rf = as_learner(imputer %>>% rf)
set.seed(2022)
res_rf = resample(learner = rf, task = task_spanish, resampling = rdesc)
res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn")
knn = as_learner(imputer %>>% knn)
set.seed(2022)
res_knn= resample(learner = knn, task = task_spanish, resampling = rdesc)
res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes")
nb = as_learner(imputer %>>% nb)
set.seed(2022)
res_nb= resample(learner = nb, task = task_spanish, resampling = rdesc)
res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_spanish, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_spanish,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(mlr3verse)
library(rpart.plot)
library(paradox)
library(future)
set.seed(2022)
learner = lrn("classif.ranger",
mtry        = to_tune(1,5),
min.node.size = to_tune(1,5)
)
at = auto_tuner(
method = tnr("random_search"),
learner = learner,
resampling = rsmp("cv", folds = 3),
measure = msr("classif.acc"),
terminator = trm("evals", n_evals = 20)
)
task = task_spanish_0
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
extract_inner_tuning_results(rr)
rr$aggregate()
at$train(task_spanish_0)
saveRDS(at,"rf_spanish.rds")
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
loaded_tuned_model <- readRDS("rf_spanish.rds")
chinese <- chinese %>%
drop_na()
chinese %>%
anyNA()
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = chinese))
# Print predictions
chinese_results %>%
slice_head(n = 5)
# Confusion matrix
#install.packages("yardstick")
library(yardstick)
chinese_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
chinese_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = chinese_results, truth = emotion, estimate = ...2) %>%
summary()
english <-read_csv("English.csv")
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
loaded_tuned_model <- readRDS("rf_spanish.rds")
english <- english %>%
drop_na()
english %>%
anyNA()
english_results <- english%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = english))
# Print predictions
english_results %>%
slice_head(n = 5)
# Confusion matrix
english_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
english_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = english_results, truth = emotion, estimate = ...2) %>%
summary()
danish <-read_csv("Danish.csv")
danish = subset(danish,select = -c(nationality,ID))
danish$emotion = as.factor(danish$emotion)
loaded_tuned_model <- readRDS("rf_spanish.rds")
danish <- danish %>%
drop_na()
danish %>%
anyNA()
danish_results <- danish%>% select(emotion) %>%
bind_cols(loaded_tuned_model %>%
predict(newdata = danish))
# Print predictions
danish_results %>%
slice_head(n = 5)
# Confusion matrix
danish_results %>%
conf_mat(emotion, ...2)
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
danish_results %>%
conf_mat(emotion, ...2) %>%
autoplot(type = "heatmap")
#Statistical summaries for the confusion matrix
conf_mat(data = danish_results, truth = emotion, estimate = ...2) %>%
summary()
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
english_1 = english %>% drop_na()
data = subset(english, select=c("duration","duration_noSilence","flux_mean","peakFreq_mean","peakFreqVoiced_mean","emotion"))
ggpairs(data)
install.packages("ggplot2")
install.packages("ggplot2")
ggpairs(data)
#install.packages("ggplot2")
library(ggplot2)
ggpairs(data)
install_github("ggobi/ggally")
library(devtools)
install_github("ggobi/ggally")
ggpairs(data)
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
#install_github("ggobi/ggally")
ggpairs(data)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)
data = subset(danish, select=c("duration","duration_noSilence","amEnvDep_sd","fmPurity_sd"))
danish <-read_csv("Danish.csv",show_col_types = FALSE)
danish = subset(danish,select = -c(nationality,ID))
danish$emotion = as.factor(danish$emotion)
data = subset(danish, select=c("duration","duration_noSilence","amEnvDep_sd","fmPurity_sd"))
#install.packages("ggplot2")
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)
data = subset(danish, select=c("duration","duration_noSilence","amEnvDep_sd","fmPurity_sd","emotion"))
#install.packages("ggplot2")
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)
spanish <-read_csv("Spanish.csv",show_col_types = FALSE)
spanish = subset(spanish,select = -c(nationality,ID))
spanish$emotion = as.factor(spanish$emotion)
data = subset(spanish, select=c("duration","duration_noSilence","amMsFreq_sd","loudnessVoiced_sd","roughnessVoiced_sd","emotion"))
#install.packages("ggplot2")
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
data = subset(chinese, select=c("duration_noSilence","emotion"))
#install.packages("ggplot2")
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)
