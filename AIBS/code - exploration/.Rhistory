library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(312,kind = "L'Ecuyer-CMRG")
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
#install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
english_1 = english %>% drop_na()
set.seed(2022)
boruta_english_train <- Boruta(emotion ~ . , data= english_1, doTrace=2) # perform Boruta search
print(boruta_english_train)
boruta_english <- TentativeRoughFix(boruta_english_train)
print(boruta_english)
boruta_signif <- names(boruta_english_train$finalDecision[boruta_english_train$finalDecision %in% c("Confirmed", "Tentative")]) # collect Confirmed and Tentative variables
print(boruta_signif)
plot(boruta_english, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_english$ImpHistory),function(i)
boruta_english$ImpHistory[is.finite(boruta_english$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_english$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_english$ImpHistory), cex.axis = 0.7)
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
task_english_0 <- TaskClassif$new(id = "English", backend = english_1,
target = "emotion")
set.seed(2022)
rf_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
rf_classif$train(task_english_0)
rf_exp_english <- explain_mlr3(rf_classif,
data = english_1,
y = english_1$emotion,
label = "ranger explainer english", colorize = FALSE)
varimp_english <- model_parts(rf_exp_english,
type = "difference")
plot(varimp_english, show_boxplots = TRUE)
variable_importance = as.data.frame(varimp_english)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance$variable),11)
# Task definition
task_english = TaskClassif$new(id = "English", backend = english,
target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob")
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_english, resampling = rdesc)
res_featureless$prediction()$confusion
print('Featureless');print(res_featureless$aggregate(mes))
# Penalized Generalized Linear Models
library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_english, resampling = rdesc)
res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
tree = lrn("classif.rpart")
tree = as_learner(imputer %>>% tree)
set.seed(2022)
res_tree = resample(learner = tree, task = task_english, resampling = rdesc)
res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
rf = lrn("classif.ranger",num.trees = 500)
rf = as_learner(imputer %>>% rf)
set.seed(2022)
res_rf = resample(learner = rf, task = task_english, resampling = rdesc)
res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn")
knn = as_learner(imputer %>>% knn)
set.seed(2022)
res_knn= resample(learner = knn, task = task_english, resampling = rdesc)
res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes")
nb = as_learner(imputer %>>% nb)
set.seed(2022)
res_nb= resample(learner = nb, task = task_english, resampling = rdesc)
res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_english, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_english,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_english_0)
tree_exp_english <- explain_mlr3(tree_classif,
data = english_1,
y = english_1$emotion,
label = "ranger explainer english", colorize = FALSE)
varimp_english_tree <- model_parts(tree_exp_english,
type = "difference")
plot(varimp_english_tree, show_boxplots = TRUE)
variable_importance_tree = as.data.frame(varimp_english_tree)
variable_importance_tree = variable_importance_tree[order(variable_importance_tree$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance_tree$variable),11)
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree$train(task_english_0)
tree_exp_english <- explain_mlr3(tree,
data = english_1,
y = english_1$emotion,
label = "rpart explainer english", colorize = FALSE)
varimp_english_tree <- model_parts(tree_exp_english,
type = "difference")
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.rpart", num.trees = 500,
predict_type = "prob")
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_english_0)
tree_exp_english <- explain_mlr3(tree_classif,
data = english_1,
y = english_1$emotion,
label = "ranger explainer english", colorize = FALSE)
varimp_english_tree <- model_parts(tree_exp_english,
type = "difference")
plot(varimp_english_tree, show_boxplots = TRUE)
variable_importance_tree = as.data.frame(varimp_english_tree)
variable_importance_tree = variable_importance_tree[order(variable_importance_tree$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance_tree$variable),11)
saveRDS(tree,"tree_english.rds")
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
loaded_psvm_model <- readRDS("tree_english.rds")
chinese <- chinese %>%
drop_na()
chinese %>%
anyNA()
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
View(chinese)
chinese_results <- chinese%>% select(emotion)
chinese_results <- chinese%>% select = (emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>% select = emotion %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>% select(chinese$emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
loaded_psvm_model %>%
predict(new_data = chinese))
loaded_psvm_model %>%
predict(new_data = chinese)
loaded_psvm_model %>%
predict(newdata = chinese)
# Print predictions
chinese_results %>%
slice_head(n = 5)
install.packages("kernlab")
suppressPackageStartupMessages({
library(tidyverse)
library(palmerpenguins)
library(tidymodels)
})
install.packages("kernlab")
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese, type = "prob"))
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
# Print predictions
chinese_results %>%
slice_head(n = 5)
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(newdata = chinese))
View(chinese_results)
chinese_results <-bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>% bind_cols(chinese$emotion)
View(chinese_results)
# Confusion matrix
chinese_results %>%
conf_mat(emotion, .pred_class)
# Confusion matrix
chinese_results %>%
conf_mat(chinese_results[,1], chinese_results[,2])
# Confusion matrix
chinese_results %>%
conf_mat()
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
loaded_psvm_model <- readRDS("tree_english.rds")
chinese <- chinese %>%
drop_na()
chinese %>%
anyNA()
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
#install.packages("kernlab")
suppressPackageStartupMessages({
library(tidyverse)
library(palmerpenguins)
library(tidymodels)
})
install.packages("devtools")
install.packages("devtools")
devtools::install_github("ropensci/skimr")
library(skimr)
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
loaded_psvm_model <- readRDS("tree_english.rds")
chinese <- chinese %>%
drop_na()
chinese %>%
anyNA()
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>% dplyr::select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>% dplyr::select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese, type = "prob"))
chinese_results <- chinese%>% dplyr::select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese))
# Print predictions
chinese_results %>%
slice_head(n = 5)
chinese_results <- chinese%>% dplyr::select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese)) %>%
# Print predictions
chinese_results %>%
slice_head(n = 5)
chinese_results <- chinese%>% dplyr::select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(newdata = chinese))
# Print predictions
chinese_results %>%
slice_head(n = 5)
# Confusion matrix
chinese_results %>%
conf_mat(emotion, .pred_class)
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
loaded_psvm_model <- readRDS("english_svm_model.rds")
chinese <- chinese %>%
drop_na()
chinese %>%
anyNA()
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
chinese_results <- chinese%>% select(emotion) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese)) %>%
bind_cols(loaded_psvm_model %>%
predict(new_data = chinese, type = "prob"))
#install.packages("kernlab")
suppressPackageStartupMessages({
library(tidyverse)
library(tidymodels)
})
english <-read_csv("English.csv",show_col_types = FALSE)
english$emotion = as.factor(english$emotion)
english = subset(english,select = -c(nationality,ID))
# Take a peek into the data
glimpse(english)
#install.packages("devtools")
devtools::install_github("ropensci/skimr")
library(skimr)
# Do some summary statistics
english %>%
skim()
english <- english %>%
drop_na()
english %>%
anyNA()
#does not work as id & other numeric features odes not fit in one plot
#install.packages("paletteer")
library(paletteer)
# Pivot data to a long format
english_long <- english %>%
pivot_longer(c(duration_noSilence,voiced_noSilence,HNRVoiced_mean,f1_freq_mean), names_to = "predictors", values_to = "values")
# Make box plots
theme_set(theme_light())
english_long %>%
ggplot(mapping = aes(x = emotion, y = values, fill = predictors)) +
geom_boxplot() +
facet_wrap(~predictors, scales = "free") +
scale_fill_paletteer_d("nbapalettes::supersonics_holiday") +
theme(legend.position = "none")
# For reproducibility
set.seed(2056)
# Split data 70%-30% into training set and test set
english_split <- english %>%
initial_split(prop = 0.70, strata = emotion)
# Extract data in each split
english_train <- training(english_split)
english_test <- testing(english_split)
# Print the number of observations in each split
cat("Training cases: ", nrow(english_train), "\n",
"Test cases: ", nrow(english_test), sep = "")
multireg_spec <- multinom_reg(penalty = 1) %>%
set_engine("nnet") %>%
set_mode("classification")
# Train a multinomial regression model without any preprocessing
set.seed(2056)
multireg_fit <- multireg_spec %>%
fit(emotion ~ ., data = english_train)
# Print the model
multireg_fit
english_results <- english_test %>% select(emotion) %>%
bind_cols(multireg_fit %>%
predict(new_data = english_test)) %>%
bind_cols(multireg_fit %>%
predict(new_data = english_test, type = "prob"))
#install.packages("kernlab")
suppressPackageStartupMessages({
library(tidyverse)
library(tidymodels)
})
english <-read_csv("English.csv",show_col_types = FALSE)
english$emotion = as.factor(english$emotion)
english = subset(english,select = -c(nationality,ID))
# Take a peek into the data
glimpse(english)
#install.packages("devtools")
devtools::install_github("ropensci/skimr")
library(skimr)
# Do some summary statistics
english %>%
skim()
english <- english %>%
drop_na()
english %>%
anyNA()
#does not work as id & other numeric features odes not fit in one plot
#install.packages("paletteer")
library(paletteer)
# Pivot data to a long format
english_long <- english %>%
pivot_longer(c(duration_noSilence,voiced_noSilence,HNRVoiced_mean,f1_freq_mean), names_to = "predictors", values_to = "values")
# Make box plots
theme_set(theme_light())
english_long %>%
ggplot(mapping = aes(x = emotion, y = values, fill = predictors)) +
geom_boxplot() +
facet_wrap(~predictors, scales = "free") +
scale_fill_paletteer_d("nbapalettes::supersonics_holiday") +
theme(legend.position = "none")
# For reproducibility
set.seed(2056)
# Split data 70%-30% into training set and test set
english_split <- english %>%
initial_split(prop = 0.70, strata = emotion)
# Extract data in each split
english_train <- training(english_split)
english_test <- testing(english_split)
# Print the number of observations in each split
cat("Training cases: ", nrow(english_train), "\n",
"Test cases: ", nrow(english_test), sep = "")
multireg_spec <- multinom_reg(penalty = 1) %>%
set_engine("nnet") %>%
set_mode("classification")
# Train a multinomial regression model without any preprocessing
set.seed(2056)
multireg_fit <- multireg_spec %>%
fit(emotion ~ ., data = english_train)
# Print the model
multireg_fit
english_results <- english_test %>% select(emotion) %>%
bind_cols(multireg_fit %>%
predict(new_data = english_test)) %>%
bind_cols(multireg_fit %>%
predict(new_data = english_test, type = "prob"))
