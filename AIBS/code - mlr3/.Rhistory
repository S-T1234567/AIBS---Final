rf_exp_chinese <- explain_mlr3(rf_classif,
data = chinese_1,
y = chinese_1$emotion,
label = "ranger explainer chinese", colorize = FALSE)
varimp_chinese <- model_parts(rf_exp_chinese,
type = "difference")
plot(varimp_chinese, show_boxplots = TRUE)
variable_importance = as.data.frame(varimp_chinese)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance$variable),11)
# Task definition
task_chinese = TaskClassif$new(id = "chinese", backend = chinese,
target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob")
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_chinese, resampling = rdesc)
res_featureless$prediction()$confusion
print('Featureless');print(res_featureless$aggregate(mes))
# Penalized Generalized Linear Models
library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_chinese, resampling = rdesc)
res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
tree = lrn("classif.rpart")
tree = as_learner(imputer %>>% tree)
set.seed(2022)
res_tree = resample(learner = tree, task = task_chinese, resampling = rdesc)
res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
rf = lrn("classif.ranger",num.trees = 500)
rf = as_learner(imputer %>>% rf)
set.seed(2022)
res_rf = resample(learner = rf, task = task_chinese, resampling = rdesc)
res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn")
knn = as_learner(imputer %>>% knn)
set.seed(2022)
res_knn= resample(learner = knn, task = task_chinese, resampling = rdesc)
res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes")
nb = as_learner(imputer %>>% nb)
set.seed(2022)
res_nb= resample(learner = nb, task = task_chinese, resampling = rdesc)
res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_chinese, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_chinese,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_chinese_0)
tree_exp_chinese <- explain_mlr3(tree_classif,
data = chinese_1,
y = chinese_1$emotion,
label = "ranger explainer chinese", colorize = FALSE)
varimp_chinese_tree <- model_parts(tree_exp_chinese,
type = "difference")
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(312,kind = "L'Ecuyer-CMRG")
danish <-read_csv("Danish.csv",show_col_types = FALSE)
danish = subset(danish,select = -c(nationality,ID))
danish$emotion = as.factor(danish$emotion)
#install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
danish_1 = danish %>% drop_na()
set.seed(2022)
boruta_danish_train <- Boruta(emotion ~ . , data= danish_1, doTrace=2) # perform Boruta search
print(boruta_danish_train)
boruta_danish <- TentativeRoughFix(boruta_danish_train)
print(boruta_danish)
boruta_signif <- names(boruta_danish_train$finalDecision[boruta_danish_train$finalDecision %in% c("Confirmed", "Tentative")]) # collect Confirmed and Tentative variables
print(boruta_signif)
plot(boruta_danish, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_danish$ImpHistory),function(i)
boruta_danish$ImpHistory[is.finite(boruta_danish$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_danish$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_danish$ImpHistory), cex.axis = 0.7)
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
task_danish_0 <- TaskClassif$new(id = "danish", backend = danish_1,
target = "emotion")
set.seed(2022)
rf_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
rf_classif$train(task_danish_0)
rf_exp_danish <- explain_mlr3(rf_classif,
data = danish_1,
y = danish_1$emotion,
label = "ranger explainer danish", colorize = FALSE)
varimp_danish <- model_parts(rf_exp_danish,
type = "difference")
setwd("~/Documents/GitHub/AIBS---Final/AIBS/code - mlr3")
plot(varimp_danish, show_boxplots = TRUE)
variable_importance = as.data.frame(varimp_danish)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance$variable),11)
# Task definition
task_danish = TaskClassif$new(id = "danish", backend = danish,
target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob")
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_danish, resampling = rdesc)
res_featureless$prediction()$confusion
print('Featureless');print(res_featureless$aggregate(mes))
# Penalized Generalized Linear Models
library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_danish, resampling = rdesc)
res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
tree = lrn("classif.rpart")
tree = as_learner(imputer %>>% tree)
set.seed(2022)
res_tree = resample(learner = tree, task = task_danish, resampling = rdesc)
res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
rf = lrn("classif.ranger",num.trees = 500)
rf = as_learner(imputer %>>% rf)
set.seed(2022)
res_rf = resample(learner = rf, task = task_danish, resampling = rdesc)
res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn")
knn = as_learner(imputer %>>% knn)
set.seed(2022)
res_knn= resample(learner = knn, task = task_danish, resampling = rdesc)
res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes")
nb = as_learner(imputer %>>% nb)
set.seed(2022)
res_nb= resample(learner = nb, task = task_danish, resampling = rdesc)
res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_danish, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_danish,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_danish_0)
tree_exp_danish <- explain_mlr3(tree_classif,
data = danish_1,
y = danish_1$emotion,
label = "ranger explainer danish", colorize = FALSE)
varimp_danish_tree <- model_parts(tree_exp_danish,
type = "difference")
setwd("~/Documents/GitHub/AIBS---Final/AIBS/code - mlr3")
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_danish_0)
tree_exp_danish <- explain_mlr3(tree_classif,
data = danish_1,
y = danish_1$emotion,
label = "ranger explainer danish", colorize = FALSE)
varimp_danish_tree <- model_parts(tree_exp_danish,
type = "difference")
plot(varimp_danish_tree, show_boxplots = TRUE)
variable_importance_tree = as.data.frame(varimp_danish_tree)
variable_importance_tree = variable_importance_tree[order(variable_importance_tree$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance_tree$variable),11)
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(312,kind = "L'Ecuyer-CMRG")
spanish <-read_csv("Spanish.csv",show_col_types = FALSE)
spanish = subset(spanish,select = -c(nationality,ID))
spanish$emotion = as.factor(spanish$emotion)
#install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
spanish_1 = spanish %>% drop_na()
set.seed(2022)
boruta_spanish_train <- Boruta(emotion ~ . , data= spanish_1, doTrace=2) # perform Boruta search
print(boruta_spanish_train)
boruta_spanish <- TentativeRoughFix(boruta_spanish_train)
print(boruta_spanish)
boruta_signif <- names(boruta_spanish_train$finalDecision[boruta_spanish_train$finalDecision %in% c("Confirmed", "Tentative")]) # collect Confirmed and Tentative variables
print(boruta_signif)
plot(boruta_spanish, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_spanish$ImpHistory),function(i)
boruta_spanish$ImpHistory[is.finite(boruta_spanish$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_spanish$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_spanish$ImpHistory), cex.axis = 0.7)
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
task_spanish_0 <- TaskClassif$new(id = "spanish", backend = spanish_1,
target = "emotion")
set.seed(2022)
rf_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
rf_classif$train(task_spanish_0)
rf_exp_spanish <- explain_mlr3(rf_classif,
data = spanish_1,
y = spanish_1$emotion,
label = "ranger explainer spanish", colorize = FALSE)
varimp_spanish <- model_parts(rf_exp_spanish,
type = "difference")
plot(varimp_spanish, show_boxplots = TRUE)
variable_importance = as.data.frame(varimp_spanish)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance$variable),11)
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob")
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_spanish, resampling = rdesc)
res_featureless$prediction()$confusion
print('Featureless');print(res_featureless$aggregate(mes))
# Penalized Generalized Linear Models
library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_spanish, resampling = rdesc)
res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
tree = lrn("classif.rpart")
tree = as_learner(imputer %>>% tree)
set.seed(2022)
res_tree = resample(learner = tree, task = task_spanish, resampling = rdesc)
res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
rf = lrn("classif.ranger",num.trees = 500)
rf = as_learner(imputer %>>% rf)
set.seed(2022)
res_rf = resample(learner = rf, task = task_spanish, resampling = rdesc)
res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn")
knn = as_learner(imputer %>>% knn)
set.seed(2022)
res_knn= resample(learner = knn, task = task_spanish, resampling = rdesc)
res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes")
nb = as_learner(imputer %>>% nb)
set.seed(2022)
res_nb= resample(learner = nb, task = task_spanish, resampling = rdesc)
res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet")
nn = as_learner(imputer %>>% nn)
set.seed(2022)
res_nn= resample(learner = nn, task = task_spanish, resampling = rdesc)
res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
design_classif = benchmark_grid(
tasks = task_spanish,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)
set.seed(2022)
tree_classif <- lrn("classif.ranger", num.trees = 500,
predict_type = "prob")
tree_classif$train(task_spanish_0)
tree_exp_spanish <- explain_mlr3(tree_classif,
data = spanish_1,
y = spanish_1$emotion,
label = "ranger explainer spanish", colorize = FALSE)
varimp_spanish_tree <- model_parts(tree_exp_spanish,
type = "difference")
plot(varimp_spanish_tree, show_boxplots = TRUE)
variable_importance_tree = as.data.frame(varimp_spanish_tree)
variable_importance_tree = variable_importance_tree[order(variable_importance_tree$dropout_loss, decreasing = TRUE),]
head(unique(variable_importance_tree$variable),11)
