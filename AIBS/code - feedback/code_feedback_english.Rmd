---
title: "code-feedback"
author: "jingxuan song"
date: "`r Sys.Date()`"
output: html_document
---
```{r}
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
```

```{r}
library(parallel)
detectCores()
cores = detectCores() - 1
```

```{r}
library(future)
plan("multisession",workers = cores)
set.seed(2022,kind = "L'Ecuyer-CMRG")
```

```{r}
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english, select = -c(nationality))
english$emotion = as.factor(english$emotion)
english <- english %>% drop_na()
```


```{r}
english.pr <- prcomp(english[c(1:163)], center = TRUE,scale. = TRUE)
summary(english.pr)
```

```{r}
screeplot(english.pr, type = "l", npcs = 25, main = "Screeplot of the first 25 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro <- cumsum(english.pr$sdev^2 / sum(english.pr$sdev^2))
plot(cumpro[0:25], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 17, col="blue", lty=5)
abline(h = 0.9, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC17"),
       col=c("blue"), lty=5, cex=0.6)
```
We notice is that the first 17 components explains more than 90% of variance, this is great! We can effectively reduce dimensionality from 165 to 17 while only “loosing” about 10% of variance!

```{r}
components <- cbind(emotion = english[, "emotion"], ID = english[,"ID"], english.pr$x[, 1:17]) %>%
as.data.frame()
```

```{r}
# Task definition
rdesc = rsmp("repeated_cv",repeats = 10, folds = 5)
mes = msrs(c("classif.ce","classif.acc"))

task_english_pca = TaskClassif$new(id = "English", backend = components, 
  target = "emotion")
task_english_pca$col_roles$group = "ID"
task_english_pca$col_roles$feature = setdiff(task_english_pca$col_roles$feature, "ID")

```


```{r}
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)

plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob") 
lasso = lrn("classif.glmnet",  alpha = 1)
tree = lrn("classif.rpart") 
rf = lrn("classif.ranger",num.trees = 500) 
knn = lrn("classif.kknn") 
nb = lrn("classif.naive_bayes") 
nn = lrn("classif.nnet") 

design_classif = benchmark_grid(
  tasks = task_english_pca,
  learners = list(featureless, lasso, tree, rf, knn, nb, nn),
  resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
```

```{r}
#install.packages("randomForest")
#install.packages("varImp")
library(caret)
library(randomForest)
library(varImp)
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter

varImpPlot(regressor)
```

```{r}
english.cf = cforest(emotion ~ ., data = english ,control = cforest_unbiased(mtry = 2, ntree = 50))
set.seed(2022)
varImp(object = english.cf,conditional = TRUE)

```


