---
author: "jingxuan song"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
```

```{r}

library(parallel)
detectCores()
cores = detectCores() - 1

```

```{r}
library(future)
plan("multisession",workers = cores)
set.seed(312,kind = "L'Ecuyer-CMRG")

```

```{r}
danish <-read_csv("Danish.csv",show_col_types = FALSE)
danish = subset(danish,select = -c(nationality,ID))
danish$emotion = as.factor(danish$emotion)
```

```{r}
#install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
danish_1 = danish %>% drop_na()
set.seed(2022)
boruta_danish_train <- Boruta(emotion ~ . , data= danish_1, doTrace=2) # perform Boruta search
print(boruta_danish_train)

boruta_danish <- TentativeRoughFix(boruta_danish_train)
print(boruta_danish)
boruta_signif <- names(boruta_danish_train$finalDecision[boruta_danish_train$finalDecision %in% c("Confirmed", "Tentative")]) # collect Confirmed and Tentative variables
print(boruta_signif)
plot(boruta_danish, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_danish$ImpHistory),function(i)
boruta_danish$ImpHistory[is.finite(boruta_danish$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_danish$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_danish$ImpHistory), cex.axis = 0.7)
```
```{r}
library(rpart.plot)
library(mlr3verse)
library(DALEXtra)
library(ggplot2)

task_danish_0 <- TaskClassif$new(id = "danish", backend = danish_1, 
  target = "emotion")
set.seed(2022)
rf_classif <- lrn("classif.ranger", num.trees = 500, 
  predict_type = "prob")
rf_classif$train(task_danish_0)

rf_exp_danish <- explain_mlr3(rf_classif, 
  data = danish_1,
  y = danish_1$emotion, 
  label = "ranger explainer danish", colorize = FALSE)

varimp_danish <- model_parts(rf_exp_danish,
  type = "difference")
plot(varimp_danish, show_boxplots = TRUE)

variable_importance = as.data.frame(varimp_danish)
variable_importance = variable_importance[order(variable_importance$dropout_loss, decreasing = TRUE),]

head(unique(variable_importance$variable),11)
```

```{r}
# Task definition
task_danish = TaskClassif$new(id = "danish", backend = danish, 
  target = "emotion")
imputer = po("imputemedian")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
```


```{r}
# Featureless
featureless = lrn("classif.featureless",predict_type = "prob") 
featureless = as_learner(imputer %>>% featureless)
set.seed(2022)
res_featureless = resample(learner = featureless, task = task_danish, resampling = rdesc)

res_featureless$prediction()$confusion

print('Featureless');print(res_featureless$aggregate(mes))
```

```{r}
# Penalized Generalized Linear Models

library(rpart.plot)
library(paradox)
library(future)
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
lasso = as_learner(imputer %>>% lasso)
set.seed(2022)
res_lasso = resample(learner = lasso, task = task_danish, resampling = rdesc)

res_lasso$prediction()$confusion
print('Lasso');print(res_lasso$aggregate(mes))
#autoplot(res, measure = msr("classif.acc"))

```



```{r}
# Single classification tree
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)

tree = lrn("classif.rpart") 
tree = as_learner(imputer %>>% tree)

set.seed(2022)
res_tree = resample(learner = tree, task = task_danish, resampling = rdesc)

res_tree$prediction()$confusion
print('Single Classification Tree');print(res_tree$aggregate(mes))
```

```{r}
# Random Forest
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)

rf = lrn("classif.ranger",num.trees = 500) 
rf = as_learner(imputer %>>% rf) 
set.seed(2022)
res_rf = resample(learner = rf, task = task_danish, resampling = rdesc)

res_rf$prediction()$confusion
print('Random Forest');print(res_rf$aggregate(mes))
```


```{r}
# K-Nearest Neighbors
#install.packages("kknn")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(kknn)
knn = lrn("classif.kknn") 
knn = as_learner(imputer %>>% knn)

set.seed(2022)
res_knn= resample(learner = knn, task = task_danish, resampling = rdesc)

res_knn$prediction()$confusion
print('K-Nearest Neighbors');print(res_knn$aggregate(mes))
```

```{r}
# Naive Bayes Classification
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
nb = lrn("classif.naive_bayes") 
nb = as_learner(imputer %>>% nb)

set.seed(2022)
res_nb= resample(learner = nb, task = task_danish, resampling = rdesc)

res_nb$prediction()$confusion
print('Naive Bayes Classification');print(res_nb$aggregate(mes))
```

```{r}
# Neural Network
#install.packages("nnet")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
library(nnet)
nn = lrn("classif.nnet") 
nn = as_learner(imputer %>>% nn)

set.seed(2022)
res_nn= resample(learner = nn, task = task_danish, resampling = rdesc)


res_nn$prediction()$confusion
print('Neural Network');print(res_nn$aggregate(mes))
```

```{r}

library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)

plan("multisession", workers = cores)
set.seed(2022)

design_classif = benchmark_grid(
  tasks = task_danish,
  learners = list(featureless, lasso, tree, rf, knn, nb, nn),
  resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
```

```{r}
library(mlr3verse)
library(rpart.plot)
library(paradox)
library(future)
set.seed(2022)
learner = lrn("classif.rpart",
  cp        = to_tune(0.01,0.1),
  minsplit  = to_tune(2, 20),
  maxdepth  = to_tune(10,20)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 20)
)

task = task_danish
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
extract_inner_tuning_results(rr)
rr$aggregate()
at$train(task_danish)
```


```{r}
saveRDS(at,"tree_danish.rds")
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese,select = -c(nationality,ID))
chinese$emotion = as.factor(chinese$emotion)
```

```{r}
loaded_tuned_model <- readRDS("tree_danish.rds") 
```

```{r}
chinese <- chinese %>% 
  drop_na()

chinese %>% 
  anyNA()
```


```{r}
chinese_results <- chinese%>% select(emotion) %>% 
  bind_cols(loaded_tuned_model %>% 
              predict(newdata = chinese))

# Print predictions
chinese_results %>% 
  slice_head(n = 5)

```

```{r}
# Confusion matrix
#install.packages("yardstick")
library(yardstick)
chinese_results %>% 
  conf_mat(emotion, ...2)
```

```{r}
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
chinese_results %>% 
  conf_mat(emotion, ...2) %>% 
  autoplot(type = "heatmap")
```

```{r}
#Statistical summaries for the confusion matrix
conf_mat(data = chinese_results, truth = emotion, estimate = ...2) %>% 
  summary()

```

```{r}
spanish <-read_csv("Spanish.csv")
spanish = subset(spanish,select = -c(nationality,ID))
spanish$emotion = as.factor(spanish$emotion)
```

```{r}
loaded_tuned_model <- readRDS("tree_danish.rds") 
```

```{r}
spanish <- spanish %>% 
  drop_na()

spanish %>% 
  anyNA()
```


```{r}
spanish_results <- spanish%>% select(emotion) %>% 
  bind_cols(loaded_tuned_model %>% 
              predict(newdata = spanish))

# Print predictions
spanish_results %>% 
  slice_head(n = 5)

```

```{r}
# Confusion matrix
spanish_results %>% 
  conf_mat(emotion, ...2)
```

```{r}
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
spanish_results %>% 
  conf_mat(emotion, ...2) %>% 
  autoplot(type = "heatmap")
```

```{r}
#Statistical summaries for the confusion matrix
conf_mat(data = spanish_results, truth = emotion, estimate = ...2) %>% 
  summary()

```

```{r}
english <-read_csv("English.csv")
english = subset(english,select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
```

```{r}
loaded_tuned_model <- readRDS("tree_danish.rds") 
```

```{r}
english <- english %>% 
  drop_na()

english %>% 
  anyNA()
```


```{r}
english_results <- english%>% select(emotion) %>% 
  bind_cols(loaded_tuned_model %>% 
              predict(newdata = english)) 

# Print predictions
english_results %>% 
  slice_head(n = 5)

```

```{r}
# Confusion matrix
english_results %>% 
  conf_mat(emotion, ...2)
```

```{r}
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
english_results %>% 
  conf_mat(emotion, ...2) %>% 
  autoplot(type = "heatmap")
```

```{r}
#Statistical summaries for the confusion matrix
conf_mat(data = english_results, truth = emotion, estimate = ...2) %>% 
  summary()

```

```{r}
data = subset(danish, select=c("duration","duration_noSilence","amEnvDep_sd","fmPurity_sd","emotion"))
#install.packages("ggplot2")
library(ggplot2)
library(devtools)
#install_github("ggobi/ggally")
library(GGally)
ggpairs(data)

```