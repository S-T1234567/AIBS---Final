tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_english_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(2022,kind = "L'Ecuyer-CMRG")
spanish <-read_csv("Spanish.csv",show_col_types = FALSE)
spanish = subset(spanish, select = -c(nationality))
spanish$emotion = as.factor(spanish$emotion)
spanish <- spanish %>% drop_na()
spanish.pr <- prcomp(spanish[c(1:163)], center = TRUE,scale. = TRUE)
summary(spanish.pr)
screeplot(spanish.pr, type = "l", npcs = 25, main = "Screeplot of the first 20 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
cumpro <- cumsum(spanish.pr$sdev^2 / sum(spanish.pr$sdev^2))
plot(cumpro[0:25], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 6, col="blue", lty=5)
abline(h = 0.9, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
components <- cbind(emotion = spanish[, "emotion"], spanish.pr$x[, 1:20]) %>%
as.data.frame()
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
task_spanish_pca = TaskClassif$new(id = "spanish", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_spanish_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
components <- cbind(emotion = spanish[, "emotion"], spanish.pr$x[, 1:10]) %>%
as.data.frame()
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
rdesc = rsmp("repeated_cv", repeats = 10,folds = 5)
mes = msrs(c("classif.ce","classif.acc"))
task_spanish_pca = TaskClassif$new(id = "spanish", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_spanish_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
rdesc = rsmp("cv", folds = 3)
mes = msrs(c("classif.ce","classif.acc"))
task_spanish_pca = TaskClassif$new(id = "spanish", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_spanish_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(2022,kind = "L'Ecuyer-CMRG")
chinese <-read_csv("Chinese.csv",show_col_types = FALSE)
chinese = subset(chinese, select = -c(nationality))
chinese$emotion = as.factor(chinese$emotion)
chinese <- chinese %>% drop_na()
chinese.pr <- prcomp(chinese[c(1:163)], center = TRUE,scale. = TRUE)
summary(chinese.pr)
screeplot(chinese.pr, type = "l", npcs = 25, main = "Screeplot of the first 20 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
cumpro <- cumsum(chinese.pr$sdev^2 / sum(chinese.pr$sdev^2))
plot(cumpro[0:25], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 6, col="blue", lty=5)
abline(h = 0.9, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
components <- cbind(emotion = chinese[, "emotion"], chinese.pr$x[, 1:20]) %>%
as.data.frame()
# Task definition
task_chinese = TaskClassif$new(id = "chinese", backend = chinese,
target = "emotion")
rdesc = rsmp("cv", folds = 3)
mes = msrs(c("classif.ce","classif.acc"))
task_chinese_pca = TaskClassif$new(id = "chinese", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_chinese_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
components <- cbind(emotion = chinese[, "emotion"], chinese.pr$x[, 1:15]) %>%
as.data.frame()
# Task definition
task_chinese = TaskClassif$new(id = "chinese", backend = chinese,
target = "emotion")
rdesc = rsmp("cv", folds = 3)
mes = msrs(c("classif.ce","classif.acc"))
task_chinese_pca = TaskClassif$new(id = "chinese", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_chinese_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
library(parallel)
detectCores()
cores = detectCores() - 1
library(future)
plan("multisession",workers = cores)
set.seed(2022,kind = "L'Ecuyer-CMRG")
danish <-read_csv("Danish.csv",show_col_types = FALSE)
danish = subset(danish, select = -c(nationality))
danish$emotion = as.factor(danish$emotion)
danish <- danish %>% drop_na()
danish.pr <- prcomp(danish[c(1:163)], center = TRUE,scale. = TRUE)
summary(danish.pr)
screeplot(danish.pr, type = "l", npcs = 25, main = "Screeplot of the first 20 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
col=c("red"), lty=5, cex=0.6)
cumpro <- cumsum(danish.pr$sdev^2 / sum(danish.pr$sdev^2))
plot(cumpro[0:25], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 6, col="blue", lty=5)
abline(h = 0.9, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
col=c("blue"), lty=5, cex=0.6)
components <- cbind(emotion = danish[, "emotion"], danish.pr$x[, 1:15]) %>%
as.data.frame()
# Task definition
task_danish = TaskClassif$new(id = "danish", backend = danish,
target = "emotion")
rdesc = rsmp("cv",folds = 3)
mes = msrs(c("classif.ce","classif.acc"))
task_danish_pca = TaskClassif$new(id = "danish", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_danish_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
plan('sequential')
autoplot(bm_classif, measure = msr("classif.acc"))
components <- cbind(emotion = spanish[, "emotion"], spanish.pr$x[, 1:8]) %>%
as.data.frame()
# Task definition
task_spanish = TaskClassif$new(id = "spanish", backend = spanish,
target = "emotion")
rdesc = rsmp("cv", folds = 3)
mes = msrs(c("classif.ce","classif.acc"))
task_spanish_pca = TaskClassif$new(id = "spanish", backend = components,
target = "emotion")
library(parallel)
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
library(future)
plan("multisession", workers = cores)
set.seed(2022)
featureless = lrn("classif.featureless",predict_type = "prob")
lasso = lrn("classif.cv_glmnet", s = "lambda.min", alpha = 1)
tree = lrn("classif.rpart")
rf = lrn("classif.ranger",num.trees = 500)
knn = lrn("classif.kknn")
nb = lrn("classif.naive_bayes")
nn = lrn("classif.nnet")
design_classif = benchmark_grid(
tasks = task_spanish_pca,
learners = list(featureless, lasso, tree, rf, knn, nb, nn),
resamplings = rdesc)
bm_classif = benchmark(design_classif)
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english, select = -c(nationality))
english$emotion = as.factor(english$emotion)
english <- english %>% drop_na()
library(caret)
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(varImp)
install.packages("varImp")
library(varImp)
regressor <- randomForest(emotion ~ . , english, importance=TRUE) # fit the random forest with default parameter
varImp(regressor) # get variable importance, based on mean decrease in accuracy
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter
varImp(regressor) # get variable importance, based on mean decrease in accuracy
randomForest$importance(regressor) # get variable importance, based on mean decrease in accuracy
randomForest::importance(regressor) # get variable importance, based on mean decrease in accuracy
randomForest::importance(regressor,conditional=TRUE) # get variable importance, based on mean decrease in accuracy
varimpAUC(regressor) # more robust towards class imbalance
randomForest::auc(regressor)
randomForest::impauc(regressor)
varImpAUC(regressor) # more robust towards class imbalance
varImp(regressor)
View(regressor)
varImpPlot(regressor)
varImpAUC(regressor) # more robust towards class imbalance
library(caret)
varImpAUC(regressor) # more robust towards class imbalance
varImpAUC(regressor$predicted) # more robust towards class imbalance
varImp(regressor) # get variable importance, based on mean decrease in accuracy
View(regressor)
regressor$importance
variable_importance = regressor$importance
View(variable_importance)
variable_importance(sort("MeanDecreaseAccuracy",decreasing = TRUE))
variable_importance[sort("MeanDecreaseAccuracy",decreasing = TRUE)]
variable_importance %>% sort("MeanDecreaseAccuracy",decreasing = TRUE)]
variable_importance %>% sort("MeanDecreaseAccuracy",decreasing = TRUE)
sort(variable_importance,"MeanDecreaseAccuracy",decreasing = TRUE)
sort(variable_importance,decreasing = TRUE)
sort(variable_importance[,4],decreasing = TRUE)
varImpRanger(emotion ~., data=english, emotion, nperm = 1, measure = "multiclass.Brier")
varImpRanger(regressor, data=english, emotion, nperm = 1, measure = "multiclass.Brier")
install.packages("randomForest")
install.packages("varImp")
library(caret)
library(randomForest)
library(varImp)
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter
varImpPlot(regressor)
install.packages("randomForest")
varImp::varImpAUC(regressor)
varImpPlot(regressor)
library(varImp)
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter
library(randomForest)
rfNews()
#install.packages("randomForest")
#install.packages("varImp")
library(caret)
#install.packages("randomForest")
#install.packages("varImp")
library(caret)
library(randomForest)
library(varImp)
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter
varImpPlot(regressor)
english.cf = cforest(emotion ~ ., data = emotion,control = cforest_unbiased(mtry = 2, ntree = 50))
english.cf = cforest(emotion ~ ., data = english,control = cforest_unbiased(mtry = 2, ntree = 50))
View(english)
english <-read_csv("English.csv",show_col_types = FALSE)
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english, select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
english <- english %>% drop_na()
#install.packages("randomForest")
#install.packages("varImp")
library(caret)
library(randomForest)
library(varImp)
regressor <- randomForest(emotion ~ . , data = english, importance=TRUE) # fit the random forest with default parameter
varImpPlot(regressor)
english.cf = cforest(emotion ~ ., data = english ,control = cforest_unbiased(mtry = 2, ntree = 50))
set.seed(2022)
varImpAUC(object = english.cf)
varImp(object = english.cf)
varImp(object = english.cf,measure = "multiclass.Brier")
varimp(object = english.cf,measure = "multiclass.Brier")
varImp(object = english.cf,measure = "multiclass.Brier")
varImp::varImp(object = english.cf,measure = "multiclass.Brier")
varImp::varImp(object = english.cf,measure = "multiclass.Brier",conditional = TRUE)
varImp::varImp(object = english.cf,conditional = TRUE)
varImp(object = english.cf,conditional = TRUE)
library(ggplot2)
library(patchwork)
library(readr)
library(MASS)
library(caTools)
library(mlr3verse)
#install.packages("iml")
library(iml)
library(mlr3)
library(mlr3learners)
library(dplyr)
library(tidyverse)
english <-read_csv("English.csv",show_col_types = FALSE)
setwd("~/Documents/GitHub/AIBS---Final/AIBS/code - feedback")
english <-read_csv("English.csv",show_col_types = FALSE)
english = subset(english, select = -c(nationality,ID))
english$emotion = as.factor(english$emotion)
english <- english %>% drop_na()
set.seed(123)
boruta.train <- Boruta(LoanStatus~.-LoanID, data = traindata, doTrace = 2)
library(Boruta)
boruta.train <- Boruta(LoanStatus~.-LoanID, data = traindata, doTrace = 2)
boruta.train <- Boruta(emotion~., data = english, doTrace = 2)
print(boruta.train)
plot(boruta.train, xlab = "", xaxt = "n")
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)
final.boruta <- TentativeRoughFix(boruta.train)
print(final.boruta)
getSelectedAttributes(final.boruta, withTentative = F)
boruta.df <- attStats(final.boruta)
View(boruta.df)
library(Boruta)
set.seed(2022)
boruta.train <- Boruta(emotion~., data = english, doTrace = 2)
print(boruta.train)
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)
final.boruta <- TentativeRoughFix(boruta.train)
print(final.boruta)
getSelectedAttributes(final.boruta, withTentative = F)
boruta.df <- attStats(final.boruta)
plot(boruta.df[where("Decision"=="Confirmed")])
plot(boruta.df["Decision"=="Confirmed"])
boruta.df["Decision"=="Confirmed"]
View(boruta.df)
boruta.df["decision"=="Confirmed"]
boruta.df[boruta.df$decision=="Confirmed"]
boruta.df[boruta.df$decision == "Confirmed"]
boruta.df[,boruta.df$decision == "Confirmed"]
boruta.df$decision == "Confirmed"
boruta.df[boruta.df$decision == "Confirmed"]
boruta.df[decision == "Confirmed"]
boruta.df["decision" == "Confirmed"]
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,10]),10])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(10)
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory[1:10]), cex.axis = 0.7)
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory[,1:10]), cex.axis = 0.7)
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory[1:10,]), cex.axis = 0.7)
at = 1:ncol(boruta.train$ImpHistory[c(1:10)], cex.axis = 0.7)
at = 1:ncol(boruta.train$ImpHistory[,c(1:10)], cex.axis = 0.7)
at = 1:ncol(boruta.train$ImpHistory, cex.axis = 0.7)
plot(boruta.train, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.train$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory, cex.axis = 0.7)
